import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm
from utils.utils import save_checkpoint, load_checkpoint

def train_pix2pix(generator, discriminator, dataloader, opt_gen, opt_disc, scheduler_gen, scheduler_disc, num_epochs=100, start_epoch=1, lr=2e-4, lambda_l1=100, device="cuda"):

    # Loss functions
    criterion_gan = nn.BCEWithLogitsLoss()  
    criterion_l1 = nn.L1Loss()    # L1 loss for pixel-wise similarity

    for epoch in range(start_epoch, num_epochs+1):
        loop = tqdm(dataloader, leave=True, desc=f"Epoch [{epoch}/{num_epochs}]")

        for idx, (edges, reals) in enumerate(loop):
            edges, reals = edges.to(device), reals.to(device)

            ### Update Discriminator ###
            discriminator.train()
            generator.eval()

            # Real images (ground truth images)
            preds_real = discriminator(reals)
            # Dynamically create the real labels based on the size of preds_real
            #real_label = torch.ones_like(preds_real, device=device)
            real_label = torch.full_like(preds_real, 0.9, device=device)
            loss_disc_real = criterion_gan(preds_real, real_label)

            # Fake images (generated by generator from edge images)
            fakes = generator(edges)
            preds_fake = discriminator(fakes)
            # Dynamically create the fake labels based on the size of preds_fake
            fake_label = torch.zeros_like(preds_fake, device=device)
            loss_disc_fake = criterion_gan(preds_fake, fake_label)

            # Total discriminator loss
            loss_disc = (loss_disc_real + loss_disc_fake) / 2

            # Backprop and optimize discriminator
            opt_disc.zero_grad()
            loss_disc.backward()
            opt_disc.step()

            ### Update Generator ###
            generator.train()
            discriminator.eval()

            # Generator loss: GAN loss + L1 loss
            fakes = generator(edges)
            preds_fake = discriminator(fakes)
            loss_gan = criterion_gan(preds_fake, real_label)  # Try to fool the discriminator

            loss_l1 = criterion_l1(fakes, reals) * lambda_l1  # L1 loss

            # Total generator loss
            loss_gen = loss_gan + loss_l1

            # Backprop and optimize generator
            opt_gen.zero_grad()
            loss_gen.backward()
            opt_gen.step()

            # Update progress bar
            loop.set_description(f"Epoch [{epoch}/{num_epochs}]")
            loop.set_postfix(loss_gen=loss_gen.item(), loss_disc=loss_disc.item())

        # Step the schedulers at the end of each epoch
        if scheduler_gen and scheduler_disc:
            scheduler_gen.step()
            scheduler_disc.step()

            # Log the current learning rates
            current_lr_gen = scheduler_gen.get_last_lr()[0]
            current_lr_disc = scheduler_disc.get_last_lr()[0]
            print(f"Epoch [{epoch}/{num_epochs}] - Generator LR: {current_lr_gen:.6f}, Discriminator LR: {current_lr_disc:.6f}")

        # Save checkpoint
        if (epoch) % 10 == 0:
            save_checkpoint({
                'epoch': epoch,
                'generator_state_dict': generator.state_dict(),
                'discriminator_state_dict': discriminator.state_dict(),
                'opt_gen_state_dict': opt_gen.state_dict(),
                'opt_disc_state_dict': opt_disc.state_dict(),
                'scheduler_gen_state_dict': scheduler_gen.state_dict(),
                'scheduler_disc_state_dict': scheduler_disc.state_dict(),
            }, filename=f"pix2pix_checkpoint_epoch_{epoch}.pth.tar")